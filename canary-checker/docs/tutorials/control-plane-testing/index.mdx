---
title: Control Plane Testing with Canary Checker, Flux & Helm
---


import Install from '../../snippets/install.mdx'

# Control Plane Testing


Deploying applications with Kubernetes is easier than ever, yet developers face increasing complexity.

Kubernetes simplifies deployment, but with it comes a labyrinth of potential issues. From resource conflicts to version incompatibilities, a failure in one component can cascade. Understanding application health through metric models like RED (Requests, Errors, Duration) and USE (Utilization, Saturation, Errors) isn't always enough. Latent errors might only surface during deployment or scaling.

For example, consider deploying a stateful PostgreSQL database via Flux on AWS. Problems could arise, including:


<Screenshot img="/img/chart-test-pyramid.svg" size="550px" shadow={false} />


* Tools like `helm template` and `helm lint` can validate chart rendering and syntax, but they don't guarantee compatibility with a specific Kubernetes version or the operators running on the cluster.
* `ct install` on a `kind` or simulated cluster can verify API compatibility and ensure all resources and operators work correctly in ideal conditions.
* Deploying to a staging environment can help catch issues before they reach production, but this approach doesn't detect capacity, performance or latent errors that only surface under load.


Control plane testing can help improve resilience by continuously redeploying workloads, ensuring there is enough capacity within the system and that all operators and external dependencies are working correctly.

Canary checker is a kubernetes-native test platform that continuously runs tests using 30+ check styles against your workloads. In this tutorial, we use it to continuously verify the ability to provision and run stateful workloads in a cluster.



The [`kubernetesResource`](../../reference/kubernetes-resource) check creates kubernetes resources based on the provided manifests & perform checks on them, it has 5 lifecycle stages:

## Lifecycle

1. **Apply Static Resources** - Applies all `staticResources` that are required for all tests to pass e.g. namespaces, secrets, etc..
2. **Apply Resources** - Applies all the workloads defined in `resources`
3. **Wait** - Using the parameters defined in `waitFor`, wait for the resources to be ready using [is-healthy](https://github.com/flanksource/is-healthy)
4. **Run Checks** - Run all the `checks` against the workloads
5. **Cleanup** - Delete all the `resources` that were created during the test.


## Tutorial

### Prerequisites


:::info Prerequisites
To follow this tutorial, you need:

- A Kubernetes cluster
- [FluxCD](https://fluxcd.io/) installed

:::


1. Define the workload under test

    Before you can create a canary you should start with a working example of a resource, in this example we use a `HelmRelease` to deploy a postgres database.

    ```yaml file=flux.yaml
    ```

    Once you have verified the helm release is working on its own, you can then begin building the control plane test using `canary-checker`.



1. Install the `canary-checker` binary
    <Install/>

    :::info Operator Mode
    This tutorial uses the CLI for faster feedback, when rolling this out to production we recommend installing `canary-checker` as an [operator](../../getting-started)

1. Next create a `Canary` CustomResourceDefinition (CRD)  using the `kubernetesResource` check type, the layout of the canary is as follows:

    ```yaml title=basic-canary.yaml file=template.yaml
    ```
    <p/>

    Using the workload defined in step 1, the check definition is as follows:

    ```yaml title=basic-canary.yaml file=basic-canary.yaml
    ```
    <p/>

1. Run the test locally using `canary-checker run basic-canary.yaml`
    <p/>
    <TerminalOutput command="canary-checker run basic-canary.yaml" >
        [2m18:01:52.745[0m [92mINF[0m (k8s) Using kubeconfig /Users/moshe/.kube/config
        [2m18:01:52.749[0m [92mINF[0m Checking basic-canary.yaml, 1 checks found
        [2m18:01:55.209[0m [92mINF[0m (control-plane-tests) HelmRelease/control-plane-tests/postgresql (created) +kustomized
        [2m18:02:21.072[0m [92mINF[0m (control-plane-tests.helm-release-postgres-check) PASS duration=28321 Helm release created:
        control-plane-tests/HelmRelease/postgresql:
        health: healthy
        message: Helm install succeeded for release control-plane-tests/postgresql.v1 with chart postgresql@16.2.2
        ready: true
        status: InstallSucceeded
        control-plane-tests/HelmRepository/bitnami:
        health: unknown
        ready: true
        [2m18:02:21.073[0m [92mINF[0m 1 passed, 0 failed in 28s

    </TerminalOutput>

    <p/>
    <p>And if you run `kubectl get events` you should see:</p>

    <TerminalOutput command="kubectl get events">
        LAST SEEN   TYPE      REASON                            OBJECT                                     MESSAGE
        26m         Normal    ChartPullSucceeded                helmchart/control-plane-tests-postgresql   pulled 'postgresql' chart with version '16.2.2'
        26m         Normal    Scheduled                         pod/postgresql-0                           Successfully assigned control-plane-tests/postgresql-0 to ip-10-0-4-167.eu-west-1.compute.internal
        26m         Normal    Pulled                            pod/postgresql-0                           Container image "docker.io/bitnami/postgresql:17.2.0-debian-12-r0" already present on machine
        26m         Normal    Created                           pod/postgresql-0                           Created container postgresql
        26m         Normal    Started                           pod/postgresql-0                           Started container postgresql
        26m         Warning   Unhealthy                         pod/postgresql-0                           Readiness probe failed: 127.0.0.1:5432 - rejecting connections
        26m         Warning   Unhealthy                         pod/postgresql-0                           Readiness probe failed: 127.0.0.1:5432 - no response
        26m         Normal    Killing                           pod/postgresql-0                           Stopping container postgresql
        113s        Normal    Scheduled                         pod/postgresql-0                           Successfully assigned control-plane-tests/postgresql-0 to ip-10-0-4-167.eu-west-1.compute.internal
        112s        Normal    Pulled                            pod/postgresql-0                           Container image "docker.io/bitnami/postgresql:17.2.0-debian-12-r0" already present on machine
        112s        Normal    Created                           pod/postgresql-0                           Created container postgresql
        112s        Normal    Started                           pod/postgresql-0                           Started container postgresql
        96s         Normal    Killing                           pod/postgresql-0                           Stopping container postgresql
        26m         Normal    HelmChartCreated                  helmrelease/postgresql                     Created HelmChart/control-plane-tests/control-plane-tests-postgresql with SourceRef 'HelmRepository/control-plane-tests/bitnami'
        26m         Normal    SuccessfulCreate                  statefulset/postgresql                     create Pod postgresql-0 in StatefulSet postgresql successful
        26m         Normal    InstallSucceeded                  helmrelease/postgresql                     Helm install succeeded for release control-plane-tests/postgresql.v1 with chart postgresql@16.2.2
        26m         Normal    UninstallSucceeded                helmrelease/postgresql                     Helm uninstall succeeded for release control-plane-tests/postgresql.v1 with chart postgresql@16.2.2
        26m         Normal    HelmChartDeleted                  helmrelease/postgresql                     deleted HelmChart 'control-plane-tests/control-plane-tests-postgresql'
        116s        Normal    HelmChartCreated                  helmrelease/postgresql                     Created HelmChart/control-plane-tests/control-plane-tests-postgresql with SourceRef 'HelmRepository/control-plane-tests/bitnami'
        113s        Normal    SuccessfulCreate                  statefulset/postgresql                     create Pod postgresql-0 in StatefulSet postgresql successful
        101s        Normal    InstallSucceeded                  helmrelease/postgresql                     Helm install succeeded for release control-plane-tests/postgresql.v1 with chart postgresql@16.2.2
        96s         Warning   CalculateExpectedPodCountFailed   poddisruptionbudget/postgresql             Failed to calculate the number of expected pods: found no controllers for pod "postgresql-0"
        96s         Normal    UninstallSucceeded                helmrelease/postgresql                     Helm uninstall succeeded for release control-plane-tests/postgresql.v1 with chart postgresql@16.2.2
        95s         Normal    HelmChartDeleted                  helmrelease/postgresql                     deleted HelmChart 'control-plane-tests/control-plane-tests-postgresql'
    </TerminalOutput>
2. Add custom check

        By default `kubernetesResource`  only checks if the resource is ready. However, you can add custom checks to validate the resource further.

        For example, you can validate the PostgreSQL database is running and accepting connections, with a custom `postgres` check:


        ```yaml
        apiVersion: canaries.flanksource.com/v1
        kind: Canary
        #...
        spec:
        kubernetesResource:
            - #...
            checks:
                - postgres:
                    - name: postgres schemas check
                    url: "postgres://$(username):$(password)@postgresql.default.svc:5432/exampledb?sslmode=disable"
                    username:
                        value: admin
                    password:
                        value: qwerty123
                    # Since we just want to check if database is responding,
                    # a SELECT 1 query should suffice
                    query: SELECT 1
        ```

        :::warning Accessing variables
        This example uses the `$(username)` and `$(password)` syntax to access the `username` and `password` variables hardcoded in the `checks` section, but in a production setting, reference secrets using [`valueFrom`](/reference/env-var)
    :::

        :::tip Alternatives to custom checks
        Instead of using a custom check you can also add a standard helm test pod to your chart or define a canary inside the chart to automatically include health checks for all workloads.
    :::


3. The final test looks like:


   ```yaml  file=custom-canary.yaml
   ```

## Conclusion

Continuous testing of your control plane is essential for maintaining resilient infrastructure at scale. By implementing continuous testing with tools like Canary Checker, Flux, and Helm, you can:

- Catch breaking changes early
- Validate infrastructure changes
- Ensure security compliance
- Maintain platform stability
- Reduce incident recovery time

This proactive approach helps catch issues before they impact production environments and affect your users.

## References

- [kubernetesResource](../../reference/kubernetes-resource) Canary
- [<iconify-icon icon="lets-icons:external"/> helm lint](https://helm.sh/docs/helm/helm_lint/)
- [<iconify-icon icon="lets-icons:external"/> helm test](https://helm.sh/docs/helm/helm_test/)
- [<iconify-icon icon="lets-icons:external"/> ct install](https://github.com/helm/chart-testing/blob/main/doc/ct_install.md)
- [<iconify-icon icon="lets-icons:external"/> Flux HelmRelease](https://fluxcd.io/flux/components/helm/helmreleases/)
- [<iconify-icon icon="lets-icons:external"/> Helm Chart Tests](https://helm.sh/docs/topics/chart_tests/)
